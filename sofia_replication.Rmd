```{r setup, include=FALSE}
library(here)
library(scales)
library(tidyverse)

theme_set(theme_bw())

knitr::opts_chunk$set(echo = TRUE)
```

```{r load-counts}
install.packages("here")
library(here)
library(readr)
notes <-read_tsv("C:/Users/ds3/Downloads/community-notes-2025-group-4/data/notes.tsv")
ratings <- read_tsv("C:/Users/ds3/Downloads/community-notes-2025-group-4/data/ratings.tsv")
#9360 notes which is not 11000 so... a little less than
nrow(notes)
```

```{r-data-cleaning}
notes <- notes |> mutate(datetime = as.POSIXct(createdAtMillis / 1000, origin = "1970-01-01", tz = "UTC")) #convert to a datetime format from milliseconds
notes <- notes |> mutate(is_misleading = (misleadingOther == 1 | misleadingFactualError == 1 | misleadingManipulatedMedia ==1 | misleadingOutdatedInformation ==1 | misleadingMissingImportantContext == 1 | misleadingUnverifiedClaimAsFact == 1 | misleadingSatire == 1 ) )
notes <- notes |> mutate(is_notmisleading = (notMisleadingOther ==1 | notMisleadingFactuallyCorrect==1 | notMisleadingOutdatedButNotWhenWritten == 1 | notMisleadingClearlySatire==1 | notMisleadingPersonalOpinion ==1))
```
plot figure 2 misleading vs not misleading notes then by trustworthy and not trustworthy sources
```{r-plot-figure2}
notes$trustworthySources <- as.factor(notes$trustworthySources)
labels <- c("Misleading", "Not Misleading")
ggplot(notes, aes(x = is_misleading, fill = trustworthySources)) +
  geom_bar(stat = "count", position = "stack") +
  coord_flip() +
  theme(legend.title = element_blank()) +
  theme(axis.title = element_blank()) +
  scale_x_discrete(label = labels) +
  scale_fill_manual(labels=c("No trustworthy sources", "trustworthy sources"), values = c("yellow", "blue"))

```

figure 7 a 
```{r-fig7a}
#ccdf
colnames(notes)
colnames(ratings)
 
ratings_with_votes_per_note <- ratings |>
  select(noteId) |>
  group_by(noteId) |>
  summarize(votes_per_note = n())

joined_raings_notes <- inner_join(notes, ratings_with_votes_per_note, by = "noteId")
 
joined_raings_notes |>
  select(classification, votes_per_note) |>
  arrange(votes_per_note) |>
  group_by(classification) |>
  mutate(total_votes_each_category = sum(votes_per_note)) |>
  mutate(fraction_votes = votes_per_note/total_votes_each_category) |>
  mutate(cdf_fraction = cumsum(fraction_votes)) |>
  mutate(ccdf_percent = (1 - cdf_fraction) * 100)|>
  filter(ccdf_percent > 0) |>
  ggplot(aes(x = votes_per_note, y = ccdf_percent, color = classification)) +
  geom_line() +
  scale_y_log10(limits = c(0.01, 100), label=comma) 

```
```{r-fig7a}
ratings_2 <- ratings |> group_by(noteId) |> summarize(total_helpful = sum(helpful), total_unhelpful = sum(notHelpful)) |> mutate(total=total_helpful + total_unhelpful, ratio_helpful = total_helpful/total)
ccdf_ratio <- inner_join(notes, ratings_2) |> group_by(classification) |> arrange(ratio_helpful) |> mutate(rank=min_rank(ratio_helpful), n=n() ccdf=(1-((rank-1)/n)) *100) |> filter(ccdf != "Invalid Number")

ggplot(ccdf_ratio, aes(x=ratio_helpful, y=ccdf, color=classification)) + geom_line()
```


```{r-fig7a-badway}
joined_ratings_notes <- merge(notes, ratings, by = "noteId")
ratio <- joined_ratings_notes |> group_by(noteId, is_misleading) |> summarize(helpful_count = sum(helpful),total_votes = n()) |> select(noteId, is_misleading, helpful_count, total_votes)
ratio <- ratio |> mutate(ratio_helpful = helpful_count/total_votes) |> arrange(ratio_helpful)
ratio_ccdf <- ratio |> group_by(is_misleading) |> mutate(cdf_fraction = cumsum(ratio_helpful)/sum(ratio_helpful)) |> mutate(ccdf_fraction = (1- cdf_fraction) * 100) 
 #|> filter(ccdf_fraction > 0)


ggplot(ratio_ccdf, aes(x=ratio_helpful, y=ccdf_fraction, color=is_misleading)) +
geom_line() +
scale_y_log10(limits = c(0.01, 100), label=comma)
```

logistic regression stuff
```{r-5c-yehtut-solution}
#model <- glm(passed  ~ hours_studied, data=student_data, family=binomial)
notes_classification_word_count <- notes |>
  mutate(word_count = str_count(summary, '\\w+')) |>
  select(classification, noteId, trustworthySources, word_count) |>
  mutate(classification = case_when(
    classification == "MISINFORMED_OR_POTENTIALLY_MISLEADING" ~ "Misleading",
    classification == "NOT_MISLEADING" ~ "Not Misleading"
  ))

notes_classification_word_count |>
  drop_na(word_count) |>
  group_by(classification) |>
  arrange(word_count) |>
  # Calculate CCDF using the rank of each note
  mutate(n = n(),
         rank = row_number(),
         ccdf_percent = (1 - (rank - 1) / n) * 100) |>
  filter(ccdf_percent > 0) |>
  ggplot(aes(x = word_count, y = ccdf_percent, color = classification)) +
  geom_line() +
  scale_y_log10(limits = c(0.01, 100), label=scales::comma) +
  labs(
    x = "Word Count",
    y = "CCDF (%)"
  )

```
load in the source tweet data for log reg tease.
```{r-load-tweet-data}
source_tweets <- get(load("C:/Users/ds3/Downloads/community-notes-2025-group-4/data/source_tweets.Rdata"))
```

```{r-logreg-features}
#misleading
  #classification in notes_classification_word_count
#trustworthy sources
  #trustworthySources in notes
#text complexity -- not required 
#sentiment -- not required
#word count
  #in notes_classification_word_count
#account age
  #source_account_created_at in source_tweets
#followers
  #source_followers_count in source_tweets
#followees 
  #source friend count in source_tweets
#verified
  #source_verified in source_tweets
#i believe the depedent variable is helpful in ratings

log_reg_ds <- ratings |> select(noteId, helpful)
source_tweets_2 <- source_tweets |> select(noteId, source_account_created_at, source_followers_count, source_friends_count, source_verified )
log_reg_ds <- inner_join(log_reg_ds, source_tweets_2, by="noteId")
log_reg_ds <- inner_join(log_reg_ds, notes_classification_word_count)

#rename to misleading
log_reg_ds <- log_reg_ds |> rename(misleading = classification, verified = source_verified, followees = source_friends_count)
#take source_account_created_at and make it age
current_date <- Sys.Date()
current_year <- format(current_date, "%Y")
log_reg_ds <- log_reg_ds |> mutate(account_age = as.numeric(current_year) - as.numeric(year(log_reg_ds$source_account_created_at)))
#drop na from all of them esp in source_tweets
log_reg_ds <- drop_na(log_reg_ds)

#made misleading into a binary variable
log_reg_ds <- log_reg_ds |> mutate(misleading = if_else(misleading == "Misleading", 1, 0, ))
log_reg_ds$source_followers_count <- log(log_reg_ds$source_followers_count,10)
log_reg_ds$followees <- log(log_reg_ds$followees, 10)
#log transformed followees and followers

#zstandardize all features
log_reg_ds <- log_reg_ds |> mutate(across(c(source_followers_count,followees,word_count,account_age), scale))

```

the actual modeling
```{r-logreg}
library(broom)
library(tidyverse)
#model <- glm(passed  ~ hours_studied, data=student_data, family=binomial)
model <- glm(helpful ~ misleading + trustworthySources + word_count + account_age + followees + source_followers_count + verified, data=log_reg_ds, family=binomial)
tidy(model)

#add predictions to data for plotting
log_reg_ds <- log_reg_ds |> mutate(predicted_prob = predict(model, type="response"))
```
# plot the validate error, highlighting the value of k with the lowest average error
plot_data <- data.frame(K, avg_validate_err, se_validate_err)
ggplot(plot_data, aes(x=K, y=avg_validate_err)) +
  geom_pointrange(aes(ymin=avg_validate_err - se_validate_err,
                      ymax=avg_validate_err + se_validate_err,
                      color=avg_validate_err == min(avg_validate_err))) +
  geom_line(color = "red") + #uhhhh idk if this is good enough
  scale_x_continuous(breaks=1:12) +
  theme(legend.position="none") +
  xlab('Polynomial Degree') +
  ylab('RMSE on validation data')

```{r-plot10}
coef_values <- coef(summary(model))[, "Estimate"]
se_vector <- summary(model)$coefficients[, "Std. Error"]
coef_values <- coef_values[-1]
se_vector <- se_vector[-1]
coef_names <- names(coef(model))
coef_names <- coef_names[-1]

plot_data <- data.frame(coef_names, coef_values, se_vector)
ggplot(plot_data, aes(x=coef_names, y=coef_values)) +
  geom_pointrange(aes(ymin=coef_values - se_vector, ymax=coef_values + se_vector))

```

# Extensions/ Question 4 Answer
 1. From here think about extensions to the article. Could you have approached this problem differently with the same data? 
 Are there other ways to plot the same information? Or can you reproduce a version of the Birdwatch algorithm from
  [this paper](https://arxiv.org/pdf/2210.15723)? Each group will probably come up with different questions to ask of the data. 
  Write down the questions your group is interested in along with a plan for how you can tackle them with the data you have.

original paper research questions -- specific reasons someone writes a note? how do misleading and nonmisleading notes differ in sentiment text characteristics
--- level of consensus among users associated with social influence of the author of the source tweet . --- what characteristics of birdwatch notes are associated
with being more helpful to raters 


the original findings of the paper -- Users more frequently file birdwatch notes for misleading than not misleading tweets, birdwatch notes are more helpful to tohher users if they link to trustworthy sources and use a positive sentiment. AND the social influence of the author is associated with differences in the lelvel of user consensus. 
for influential users less oncsensus incorrect and argumentative. 



#### What We Did Differently and Why It May Be Better
why are we doing log reg to predict a continous value ?

so in our figure 10 on the logisitc regression, we predicted the probability that someone
who filed a rating for the note voted helpful or not based on the characteristics of the source tweet and its author

the original regression is predicting the ratio of helpful:not helpful votes per note (not per rating) using the same features
of the tweet and its author. I think our regression is more impactful in determining what makes someone vote helpful or not helpful in a note 
and further finds what makes a user more or less likely to agree with a note.  this could allow us to include more detailed info on the rater themselves
and attributes about their accoutn to see what type of person/account votes helpful or unhelpful

I think also a plot that talks about the characterisitcs of a note that users find helpful or not is more important than the questions
asking why its helpgul because to me all these options in the poll "informative, clear, good sources, empathetic, unique context" are all vaguely the same
like contains_url like the figure 5 plots but per rating....

#### Addiitonal Research Question
i guess a further research question could be to see if the notes affects how ppl tweets (twitter celebs + noncelebs who have gotten fact checked)
do they correct their behavior, in Johnas presentation he talked about how notes terminates the spread of misinformation but does it change
individual behavior and does it change the behavior of the tweeters or does it change other ppl who see them getting fact checked to prevent misinfo 
being posted in the first place

#### A Feature We Would Add To The Dataset
So a potential feature to Birdwatch to add that would provide more depth into the ratings system would be for each user to be able to see 
per note, the note author's profile of previous notes they have written and if they have been marked helpful or unhelpful. so when 
a user is looking at a note and deciding to rate it or not, they can see if that note author is sus.

Now this could lead to some bias where a user who has posted contraversial notes before may now never get a note put thorugh
because ppl are put off with their past note history instead of the current note they posted. 


According to figure 10 for regression it seems like linking to a trustworthy source is the most significant feature in getting a high 
helpfulness ratio, so a potential guideline is maybe to have every note required to have an outside url so a user can look for themselves.

I guess this is just a guess this is just a heuristic but to me the only way to truly verify a tweet is real is  to look at a source, 
not someone tweeting about the source or sharing their interpretation of the topic. so maybe we could use an llm (like grok) to summarize the source 
and see if it matches up with what the note says an extra check to see if the note is helpful or not? so u dont have to @ grok 

#### How We Would Plot This Data Differently

